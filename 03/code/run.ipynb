{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from micromind import MicroMind, Metric\n",
    "from micromind.networks import PhiNet\n",
    "from micromind.utils.parse import parse_arguments\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from dataset import ImageWoof\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "batch_size = 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import torch.utils.data\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import cv2\n",
    "import glob\n",
    "from PIL import Image\n",
    "import ntpath\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def START_seed():\n",
    "    seed = 9\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "lr = 0.1\n",
    "epochs=50\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_batch_size = 16\n",
    "test_batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Conv2d(3, 8, 7, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(8, 8, 7, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            #####\n",
    "            nn.Conv2d(8, 16, 5, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 16, 5, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            #####\n",
    "            nn.Conv2d(16, 32, 3, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, 3, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d(\n",
    "                output_size=8,\n",
    "            ),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(32 * 8 * 8, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.feature_extractor(x)\n",
    "        features = torch.flatten(features, start_dim=1)\n",
    "        out = self.classifier(features)\n",
    "        return out\n",
    "\n",
    "    def num_of_params(self):\n",
    "        total = 0\n",
    "        for layer_params in self.feature_extractor.parameters():\n",
    "            total += layer_params.numel()\n",
    "        for layer_params in self.classifier.parameters():\n",
    "            total += layer_params.numel()\n",
    "        return total\n",
    "\n",
    "    def compute_loss(self, pred, batch):\n",
    "        return nn.CrossEntropyLoss()(pred, batch[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9025\n",
      "902\n",
      "Trainset size:  63\n",
      "Valset size:  7\n",
      "Testset size:  30\n"
     ]
    }
   ],
   "source": [
    "model = CNN(num_classes=10)\n",
    "\n",
    "def compute_accuracy(pred, batch):\n",
    "    tmp = (pred.argmax(1) == batch[1]).float()\n",
    "    return tmp\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), \n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "        transforms.Resize((160, 160), antialias=True)\n",
    "    ]\n",
    ")\n",
    "\n",
    "trainset = ImageWoof(\n",
    "    root=\".\", train=True, transform=transform, img_size=160\n",
    ")\n",
    "testset = ImageWoof(\n",
    "    root=\".\", train=False, transform=transform, img_size=160\n",
    ")\n",
    "\n",
    "## split into train, val, test \n",
    "print(len(trainset))     \n",
    "val_size = int(0.1 * len(trainset))\n",
    "print(val_size)\n",
    "train_size = len(trainset) - val_size\n",
    "train, val = torch.utils.data.random_split(trainset, [train_size, val_size])    \n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    train, batch_size=batch_size, shuffle=True, num_workers=1\n",
    ")\n",
    "valloader = torch.utils.data.DataLoader(\n",
    "    val, batch_size=batch_size, shuffle=False, num_workers=1\n",
    ")    \n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=batch_size, shuffle=False, num_workers=1\n",
    ")\n",
    "\n",
    "print(\"Trainset size: \", len(train)//batch_size)\n",
    "print(\"Valset size: \", len(val)//batch_size)\n",
    "print(\"Testset size: \", len(testset)//batch_size)\n",
    "\n",
    "acc = Metric(name=\"accuracy\", fn=compute_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "CNN                                      [128, 10]                 --\n",
       "├─Sequential: 1-1                        [128, 32, 8, 8]           --\n",
       "│    └─Conv2d: 2-1                       [128, 8, 154, 154]        1,184\n",
       "│    └─ReLU: 2-2                         [128, 8, 154, 154]        --\n",
       "│    └─Conv2d: 2-3                       [128, 8, 148, 148]        3,144\n",
       "│    └─ReLU: 2-4                         [128, 8, 148, 148]        --\n",
       "│    └─MaxPool2d: 2-5                    [128, 8, 74, 74]          --\n",
       "│    └─Conv2d: 2-6                       [128, 16, 70, 70]         3,216\n",
       "│    └─ReLU: 2-7                         [128, 16, 70, 70]         --\n",
       "│    └─Conv2d: 2-8                       [128, 16, 66, 66]         6,416\n",
       "│    └─ReLU: 2-9                         [128, 16, 66, 66]         --\n",
       "│    └─MaxPool2d: 2-10                   [128, 16, 33, 33]         --\n",
       "│    └─Conv2d: 2-11                      [128, 32, 31, 31]         4,640\n",
       "│    └─ReLU: 2-12                        [128, 32, 31, 31]         --\n",
       "│    └─Conv2d: 2-13                      [128, 32, 29, 29]         9,248\n",
       "│    └─ReLU: 2-14                        [128, 32, 29, 29]         --\n",
       "│    └─AdaptiveAvgPool2d: 2-15           [128, 32, 8, 8]           --\n",
       "├─Sequential: 1-2                        [128, 10]                 --\n",
       "│    └─Linear: 2-16                      [128, 1024]               2,098,176\n",
       "│    └─ReLU: 2-17                        [128, 1024]               --\n",
       "│    └─Linear: 2-18                      [128, 512]                524,800\n",
       "│    └─ReLU: 2-19                        [128, 512]                --\n",
       "│    └─Linear: 2-20                      [128, 128]                65,664\n",
       "│    └─ReLU: 2-21                        [128, 128]                --\n",
       "│    └─Linear: 2-22                      [128, 10]                 1,290\n",
       "==========================================================================================\n",
       "Total params: 2,717,778\n",
       "Trainable params: 2,717,778\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 19.91\n",
       "==========================================================================================\n",
       "Input size (MB): 39.32\n",
       "Forward/backward pass size (MB): 586.13\n",
       "Params size (MB): 10.87\n",
       "Estimated Total Size (MB): 636.32\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model, input_size=(batch_size, 3, 160, 160))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 3, 160, 160])\n",
      "2\n",
      "<class 'list'>\n",
      "tensor([[ 0.0484,  0.0614, -0.0186,  ..., -0.0041,  0.0187,  0.0285],\n",
      "        [ 0.0484,  0.0614, -0.0185,  ..., -0.0041,  0.0187,  0.0285],\n",
      "        [ 0.0484,  0.0614, -0.0186,  ..., -0.0041,  0.0187,  0.0285],\n",
      "        ...,\n",
      "        [ 0.0484,  0.0614, -0.0185,  ..., -0.0039,  0.0187,  0.0285],\n",
      "        [ 0.0484,  0.0614, -0.0186,  ..., -0.0040,  0.0187,  0.0285],\n",
      "        [ 0.0483,  0.0614, -0.0186,  ..., -0.0040,  0.0187,  0.0285]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "torch.Size([128, 10])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "tensor([2, 4, 5, 5, 1, 1, 0, 0, 2, 5, 0, 4, 6, 1, 3, 6, 7, 8, 4, 6, 3, 6, 0, 9,\n",
      "        3, 4, 7, 3, 9, 6, 8, 4, 6, 9, 6, 4, 0, 4, 2, 7, 0, 3, 6, 7, 4, 1, 5, 9,\n",
      "        2, 5, 9, 5, 9, 2, 5, 5, 6, 2, 1, 6, 0, 4, 2, 1, 5, 9, 0, 6, 7, 6, 5, 8,\n",
      "        5, 9, 2, 8, 7, 0, 7, 9, 7, 1, 1, 8, 3, 1, 3, 0, 9, 0, 3, 5, 0, 0, 1, 8,\n",
      "        3, 8, 5, 6, 4, 5, 7, 1, 1, 9, 8, 8, 2, 0, 5, 0, 5, 1, 9, 9, 8, 0, 5, 8,\n",
      "        2, 5, 8, 4, 2, 8, 8, 7], device='cuda:0')\n",
      "tensor(0.1016, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(trainloader))\n",
    "data = batch[0]\n",
    "target = batch[1]\n",
    "\n",
    "target = target.to('cuda:0')\n",
    "\n",
    "print(batch[0].shape)\n",
    "print(len(batch))\n",
    "\n",
    "print(type(batch))\n",
    "\n",
    "out = m.forward(batch[0].to('cuda:0'))\n",
    "\n",
    "print(out)\n",
    "\n",
    "print(out.shape)\n",
    "\n",
    "print(torch.argmax(out, dim=1))\n",
    "print(target)\n",
    "\n",
    "corrects = target == torch.argmax(out, dim=1)\n",
    "\n",
    "accuracy = torch.sum(corrects) / len(corrects)\n",
    "\n",
    "print(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_seed()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(m.parameters(), lr=lr) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Creating training loop\n",
    "def train(model):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    total = 0\n",
    "    correct=0\n",
    "    with tqdm(trainloader, unit=\"batch\") as tepoch:\n",
    "        for batch_idx, (data, target) in enumerate(tepoch):\n",
    "            # send to device\n",
    "            data, target = data.to(device), target.to(device)                \n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            # calculate the loss\n",
    "            loss = criterion(output, target)\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            # update running training loss\n",
    "            train_loss += loss.item()*data.size(0)\n",
    "            _, predicted = output.max(1)\n",
    "            total += target.size(0)\n",
    "            correct += predicted.eq(target).sum().item()\n",
    "        print(' train loss: {:.4f} accuracy: {:.4f}'.format(train_loss/(batch_idx+1), 100.*correct/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracy = 0.0\n",
    "def validate(model):\n",
    "    global best_accuracy\n",
    "    model.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with tqdm(valloader, unit=\"batch\") as tepoch:\n",
    "        for batch_idx, (data, target) in enumerate(tepoch):            \n",
    "            data, target = data.to(device), target.to(device)    \n",
    "            output = model(data)\n",
    "            loss = nn.CrossEntropyLoss()(output, target)\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = output.max(1)\n",
    "            total += target.size(0)\n",
    "            correct += predicted.eq(target).sum().item()\n",
    "        if (100.*correct/total) > best_accuracy:\n",
    "            print(\"Saving the best model...\")\n",
    "            best_accuracy = (100.*correct/total)\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        print(' val loss: {:.4f} accuracy: {:.4f} best_accuracy: {:.4f}'.format(test_loss/(batch_idx+1), 100.*correct/total, best_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd1276f86cb741ad846079431f086b6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " train loss: 292.6050 accuracy: 10.4149\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9a55d6a2ae14c048b7bb55129a39fbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the best model...\n",
      " val loss: 2.3044 accuracy: 11.4191 best_accuracy: 11.4191\n",
      "epoch number: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aac4f4984c8b452b93ddf48cb89d6199",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " train loss: 292.6050 accuracy: 10.4149\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d88f21126efa4f2caa9b7a9de6c95d1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " val loss: 2.3044 accuracy: 11.4191 best_accuracy: 11.4191\n",
      "epoch number: 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52252e17d3d34b10898dfab3732e3433",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " train loss: 292.6050 accuracy: 10.4149\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eabf0392d0c844f98983715c358a3419",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " val loss: 2.3044 accuracy: 11.4191 best_accuracy: 11.4191\n",
      "epoch number: 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6010af9251424a3586d8418ef1e7e30a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " train loss: 292.6050 accuracy: 10.4149\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e3c2307dd784b3bba82efeae0c90cbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " val loss: 2.3044 accuracy: 11.4191 best_accuracy: 11.4191\n",
      "epoch number: 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4c746bc1e9f45d99c13ebacdb271113",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " train loss: 292.6049 accuracy: 10.4149\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b17c3ffb8ff8446da8a02301573f9fd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " val loss: 2.3044 accuracy: 11.4191 best_accuracy: 11.4191\n",
      "epoch number: 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "284d2e7e22734106a5ef36399351b523",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " train loss: 292.6050 accuracy: 10.4149\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1a379be650b47c8b6904eadd259696f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " val loss: 2.3044 accuracy: 11.4191 best_accuracy: 11.4191\n",
      "epoch number: 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06b05a09a9934414a115679832f99f5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " train loss: 292.6050 accuracy: 10.4149\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9fe96f4f116486a82517c76df48bdb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.to(device)\n",
    "start = time.time()\n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "    print(\"epoch number: {0}\".format(epoch))\n",
    "    train(model)\n",
    "    validate(model)\n",
    "end = time.time()\n",
    "Total_time=end-start\n",
    "print('Total training and inference time is: {0}'.format(Total_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_best_model(model, test_loader, criterion, best_model_path):\n",
    "    # Load the best model\n",
    "    model.load_state_dict(torch.load(best_model_path))\n",
    "    model.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with tqdm(testloader, unit=\"batch\") as tepoch:\n",
    "        for batch_idx, (data, target) in enumerate(tepoch):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            _, predicted = output.max(1)\n",
    "            total += target.size(0)\n",
    "            correct += predicted.eq(target).sum().item()\n",
    "\n",
    "        accuracy = 100. * correct / total\n",
    "        print('Test loss: {:.4f} Accuracy: {:.2f}%'.format(test_loss/(batch_idx+1), accuracy))\n",
    "\n",
    "# Usage example:\n",
    "\n",
    "# Assuming you have a model, test_loader, and best_model_path defined\n",
    "\n",
    "best_model_path = 'best_model.pth'  # Replace with the actual path and filename of the best model\n",
    "\n",
    "test_best_model(model, test_loader, nn.CrossEntropyLoss(), best_model_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai701",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
