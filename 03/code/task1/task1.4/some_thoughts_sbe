before 51%

adding the batchnormalization
-> 20 epochs 
Test loss: 1.4267 Accuracy: 61.26% model01
very good performance

adding the dropout
-> more epochs because it has to learn more
-> will see but the convergence seems slower now
-> dropout 0.5 it is too much Test loss: 2.3113 Accuracy: 21.61% Yes it doesnt learn much model02
-> I also try with 0.2 not even good, that is because there are so many dropouts! Maybe not learning much model03
-> I left them only at the very last layer model04
-> Test loss: 2.0502 Accuracy: 64.90% model04 best -> dropout 0.2 only lasst layerss